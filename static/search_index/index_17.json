{"/news/others/v831_resnet18/v831_resnet18.html": {"title": "在V831上（awnn）跑 pytorch resnet18 模型", "content": "---\ntitle: 在V831上（awnn）跑 pytorch resnet18 模型\nkeywords: V831,awnn,resnet18\ndate: 2022-06-13\ntags: V831,awnn,resnet18\n---\n\n在V831上（awnn）跑 pytorch resnet18 模型，和模型转换方法\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/358\n\n原文时间：2021.04.10， 搬运有改动\n\n- 可以参考一下\n\n## 直接使用 Pytorch hub 与模型训练\n\n此处省略模型定义和训练过程，仅使用 pytorch hub 的 resnet18 预训练模型进行简单介绍\n\nhttps://pytorch.org/hub/pytorch_vision_resnet/\n\n## 在 PC 端测试模型推理\n\n根据上面链接的使用说明，使用下面代码可以运行模型\n\n其中，label 下载：https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n```python\nimport os\nimport torch\nfrom torchsummary import summary\n\n## model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\nmodel.eval()\ninput_shape = (3, 224, 224)\nsummary(model, input_shape, device=\"cpu\")\n\n## test image\nfilename = \"out/dog.jpg\"\nif not os.path.exists(filename):\n    if not os.path.exists(\"out\"):\n        os.makedirs(\"out\")\n    import urllib\n    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n    try: urllib.URLopener().retrieve(url, filename)\n    except: urllib.request.urlretrieve(url, filename)\nprint(\"test image:\", filename)\n\n## preparing input data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\ninput_image = Image.open(filename)\n\n# input_image.show()\npreprocess = transforms.Compose([\n    transforms.Resize(max(input_shape[1:3])),\n    transforms.CenterCrop(input_shape[1:3]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\nprint(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor)))\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n## forward model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\nwith torch.no_grad():\n    output = model(input_batch)\n\n## result    \n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n# print(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nmax_1000 = torch.nn.functional.softmax(output[0], dim=0)\nmax_idx = int(torch.argmax(max_1000))\nwith open(\"imagenet_classes.txt\") as f:\n    labels = f.read().split(\"\\n\")\nprint(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx]))\n```\n\n运行结果如下：\n\n```python\nUsing cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 44.59\nEstimated Total Size (MB): 107.96\n----------------------------------------------------------------\nout/dog.jpg\ntensor(2.6400) tensor(-2.1008)\nidx:258, name:Samoyed, Samoyede\n```\n\n可以看到模型有 11,689,512 个参数，差不多 11MiB 左右，这个大小也几乎是实际在 831 上运行的模型大小了\n\n## 将模型转换为 V831 能使用的模型文件\n\n转换过程如下：\n\n使用 Pytorch 将模型导出为 onnx 模型， 得到 onnx 文件\n\n```python\ndef torch_to_onnx(net, input_shape, out_name=\"out/model.onnx\", input_names=[\"input0\"], output_names=[\"output0\"], device=\"cpu\"):\n    batch_size = 1\n    if len(input_shape) == 3:\n        x = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype=torch.float32, requires_grad=True).to(device)\n    elif len(input_shape) == 1:\n        x = torch.randn(batch_size, input_shape[0], dtype=torch.float32, requires_grad=False).to(device)\n    else:\n        raise Exception(\"not support input shape\")\n    print(\"input shape:\", x.shape)\n    # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params=True)\n    torch.onnx.export(net, x, out_name, export_params=True, input_names = input_names, output_names=output_names)\nonnx_out=\"out/resnet_1000.onnx\"\nncnn_out_param = \"out/resnet_1000.param\"\nncnn_out_bin = \"out/resnet_1000.bin\"\ninput_img = filename\ntorch_to_onnx(model, input_shape, onnx_out, device=\"cuda:0\")\n```\n\n如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字\n\n使用 onnx2ncnn 工具将 onnx 转成 ncnn 模型，得到一个 .param 文件和一个 .bin 文件\n按照 ncnn 项目的编译说明编译，在 build/tools/onnx 目录下得到 onnx2ncnn 可执行文件\n\n```python\ndef onnx_to_ncnn(input_shape, onnx=\"out/model.onnx\", ncnn_param=\"out/conv0.param\", ncnn_bin = \"out/conv0.bin\"):\n    import os\n    # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir\n    cmd = f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\"\n    os.system(cmd)\n    with open(ncnn_param) as f:\n        content = f.read().split(\"\\n\")\n        if len(input_shape) == 1:\n            content[2] += \" 0={}\".format(input_shape[0])\n        else:\n            content[2] += \" 0={} 1={} 2={}\".format(input_shape[2], input_shape[1], input_shape[0])\n        content = \"\\n\".join(content)\n    with open(ncnn_param, \"w\") as f:\n        f.write(content)\nonnx_to_ncnn(input_shape, onnx=onnx_out, ncnn_param=ncnn_out_param, ncnn_bin=ncnn_out_bin)\n\n```\n## 使用全志提供的awnn工具将ncnn模型进行量化到int8模型\n\n在 maix.sipeed.com 模型转换 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做）\n\n阅读转换说明，可以获得更多详细的转换说明\n\n![](./assets/convert.png)\n\n这里有几组参数：\n\n- 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值 - mean ) / std`, awnn 对输入的处理是 `(输入值 - mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给 awnn 量化工具经过 `(输入值 - mean ) * norm` 计算后的值范围一致既可。 比如这里打印了实际数据的输入范围是 [-2.1008, 2.6400]， 是代码中 preprocess 对象处理后得到的，即 `x = (x - mean) / std ==> (0-0.485)/0.229 = -2.1179`, 到 awnn 就是 `x = (x - mean_2*255) * (1 / std * 255)` 即 `mean2 = mean * 255`, `norm = 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result#pre-process)。\n所以我们这里可以设置 均值为 `0.485 * 255 = 123.675`， 设置 归一化因子为 `1/ (0.229 * 255) = 0.017125`， 另外两个通道同理。但是目前 awnn 只能支持三个通道值一样。。。所以填 `123.675, 123.675, 123.675，0.017125, 0.017125, 0.017125` 即可，因为这里用了 pytorch hub 的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下\n\n- 图片分辨率（问不是图片怎么办？貌似 awnn 暂时之考虑到了图片。。）\n\n- RGB 格式： 如果训练输入的图片是 RGB 就选 RGB\n- 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手\n\n自己写的其它模型转换如果失败，多半是啥算子不支持，上图框出的地方查看所支持的算子，比如现在的版本view、 flatten、reshape 都不支持所以写模型要相当小心，后面的版本会支持 flatten reshape 等 CPU 算子\n\n如果不出意外， 终于得到了量化好的 awnn 能使用的模型， *.param 和 *.bin\n\n## 使用模型，在v831上推理\n\n可以使用 python 或者 C 写代码，以下两种方式\n\n### MaixPy3\n\npython 请看 [MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/)\n\n不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了：\n\n```bash\npip install --upgrade maixpy3\n```\n\n然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）：\n\nhttps://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py\n\nlabel 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py\n\n```python\nfrom maix import nn\nfrom PIL import Image, ImageDraw\nfrom maix import camera, display\n\ntest_jpg = \"/root/test_input/input.jpg\"\nmodel = {\n    \"param\": \"/root/models/resnet_awnn.param\",\n    \"bin\": \"/root/models/resnet_awnn.bin\"\n}\n\ncamera.config(size=(224, 224))\n\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (1, 1, 1000)\n    },\n    \"first_layer_conv_no_pad\": False,\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n}\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nprint(\"-- read image\")\nimg = Image.open(test_jpg)\nprint(\"-- read image ok\")\nprint(\"-- forward model with image as input\")\nout = m.forward(img, quantize=True)\nprint(\"-- read image ok\")\nprint(\"-- out:\", out.shape)\nout = nn.F.softmax(out)\nprint(out.max(), out.argmax())\n\nfrom classes_label import labels\nwhile 1:\n    img = camera.capture()\n    if not img:\n        time.sleep(0.02)\n        continue\n    out = m.forward(img, quantize=True)\n    out = nn.F.softmax(out)\n    msg = \"{:.2f}: {}\".format(out.max(), labels[out.argmax()])\n    print(msg)\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 0), msg, fill=(255, 0, 0))\n    display.show(img)\n```\n\n### C 语言 SDK,libmiax\n\n按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet\n\n上传编译成功后dist目录下的所有内容到 v831, 然后执行./start_app.sh即可"}, "/news/others/color_introduction/color_introduction.html": {"title": "常见的图像颜色空间解释", "content": "---\ntitle: 常见的图像颜色空间解释\nkeywords: Color, 色彩空间,RGB,HSV,YUV,LAB,CMYK\ndesc: 色彩空间科普\ndate: 2022-06-11\ntags: Color, 色彩空间\ncover: ./assets/cover.png\n---\n\n常用颜色表示方法： RGB HSV YUV LAB CMYK\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/294\n\n## RGB\n\n红绿蓝 三色，也是大家熟悉光学三原色\n\nRGB 使用加色模式，也就是默认是黑色，三原色相加获得白色， 比如下图`蓝色+绿色=青色(cyan)`，得到`蓝色=青色-绿色`也就是`蓝色=青色+绿色`的互补色， 绿色的互补色就是图中的`M(品红色）magenta`（同理蓝色的互补色是Y黄色）（互补色就是两者相加为白色），所以`蓝色=青色+品红色`\n\n![](./assets/color_add.png)\n\n![](./assets/coordinate_box.png)\n\n上图可以看到灰度图在正方体对角线上，即三个通道（轴）的值相等时，值越大越白\n\n![](./assets/color.png)\n\n一般两种表示方法：\n\n### RGB888(24bit)\n\nRGB三个通道，每个通道分别用8bit长度表示，比如(255, 255, 255), 每个通道取值范围为[0, 255]，或者0xFFFFFF三个字节表示\n\n### RGB565(16bit)\n\nRGB三个通道分别用5bit 6bit 5bit表示，比如(31, 63, 31)，但一般不这样表示，使用两个字节表示，比如0xFFFF, 一般在编写程序时在内存中多使用以下两种布局方式：\n\n- 第一种：\n\n![](./assets/bgr_color.png)\n\n- 第二种：\n\n![](./assets/grgb_color.png)\n\n这两种方式的不同主要是因为 RGB565 共占用 2 个字节， 两个字节的顺序不同造成的，把第二张图从中间分隔成两份，右边移到左边，就变成了第一种的排列方式了\n\nC语言结构体如下：\n```c\n#define COLOR_16_SWAP 1\ntypedef union\n{\n    struct\n    {\n#if COLOR_16_SWAP == 0\n        uint16_t blue : 5;\n        uint16_t green : 6;\n        uint16_t red : 5;\n#else\n        uint16_t green_h : 3;\n        uint16_t red : 5;\n        uint16_t blue : 5;\n        uint16_t green_l : 3;\n#endif\n    } ch;\n    uint16_t full;\n} color16_t;\n```\n\n比如`(1,2,3)` `(R, G, B)`：\n\n使用第一种方式二进制值为 `B00001 000010 00011 ` 即 `B0000 1000 0100 0011`, 十六进制表示为 `0x0843`（注意这里表示方法从左到右是从高位到低位，上面的图从左到右是低位到高位）；\n\n使用第二种方式二进制值为 `B010 00011 00001 000 `即 `B0100 0011 0000 1000`, 十六进制表示为 `0x4308`；\n\n## HSV\n\n相关解释：\n- Hue（色调、色相）\n- Saturation（饱和度、色彩纯净度）\n- Value（明度）\n\n![](./assets/hsv.png)\n\n## HLS\n\nHLS 中的 L 分量为亮度，亮度为100，表示白色，亮度为0，表示黑色；HSV 中的 V 分量为明度，明度为100，表示光谱色，明度为0，表示黑色。\n\n提取白色物体时，使用 HLS 更方便，因为 HSV 中的Hue里没有白色，白色需要由S和V共同决定（S=0, V=100）。而在 HLS 中，白色仅由亮度L一个分量决定。所以检测白色时使用 HSL 颜色空间更准确。\n\n![](./assets/hls.png)\n\n## YUV\n\nY'UV、YUV、YCbCr、YPbPr 几个概念其实是一回事儿。Y’UV、YUV 主要是用在彩色电视中，用于模拟信号表示。YCbCr 是用在数字视频、图像的压缩和传输，如 MPEG、JPEG。今天大家所讲的 YUV 其实就是指 YCbCr。Y 表示亮度（luma），CbCr 表示色度（chroma）。\n\n另外Y’UV在取值上可以使正或者负数，但是Y’CbCr一般是 `16–235` 或者 `0–255`\n\nY’UV 设计的初衷是为了使彩色电视能够兼容黑白电视。对于黑白电视信号，只需要 Y 通道， 在彩色电视则显示 YUV 信息\n\n![](./assets/yuv.png)\n\n![](./assets/y'uv.png)\n\n### 打包格式和采样\n\nYUV 格式通常有两大类:打包(packed)格式和平面(planar)格式， 前者是每个像素为基本单位，一个一个像素的数据连续排列在内存中， 后者则是YUV 分成3个数组（内存块）存放， 另外还有Y和UV分开存放的（比如 YUV420SP（ Semi-Planar， U和V交叉放，即YYYYYYYY…UVUV…） 和 YUV420P（先放U再放V，即YYYYYYYY…UUVV））\n\n人眼的视觉特点是对亮度更铭感，对位置、色彩相对来说不铭感。在视频编码系统中为了降低带宽，可以保存更多的亮度信息(luma)，保存较少的色差信息(chroma)。这叫做 chrominance subsamping, 色度二次采样。原则：在数字图像中，(1) 每一个图形像素都要包含 luma（亮度）值；（2）几个图形像素共用一个 Cb + Cr 值，一般是 2、4、8 个像素。 一般分为以下几种\n\n- YUV444：就是每个 Y 值对应一个 U 和 一个 V 值\n- YUV422: 就是图像的横轴（width方向） 4 个 Y 值公用 2 个 U 和V，纵轴（height方向）4 个 Y 对应了 4 个 U 和 4 个 V\n- YUV420: 就是在 YUV422 的基础上， 纵轴的 U 和 V 数量也减半，每 2 个提供给 4 个 Y 配对使用\n- YUV411: 同理，就是 4 个 Y， 对应横轴和纵轴的 1 个 U 和 V\n\n![](./assets/yuv_pack.png)\n\n`SP`(Semi-Planar) 和 `P` 的说法， 区别就是在内存中的存放顺序不同, 比如\n\nYUV420SP:\n\n![](assets/yuv420sp.jpg)\n\nYUV420P:\n\n![](assets/yuv420p.jpg)\n\n另外，还有 NV12 和 NV21 的区别， 就是在 UV 的存放上顺序不同\n\n- NV12: IOS只有这一种模式。存储顺序是先存Y，再UV交替存储。YYYYUVUVUV\n- NV21: 安卓的模式。存储顺序是先存Y，再存U，再VU交替存储。YYYYVUVUVU\n\nYUV与RGB之间的转换\n\n略\n\n参考代码： https://github.com/latelee/yuv2rgb\n\n## LAB\n\nLab颜色空间中的L分量用于表示像素的亮度，取值范围是[0,100],表示从纯黑到纯白；a表示从红色到绿色的范围，取值范围是[127,-128]；b表示从黄色到蓝色的范围，取值范围是[127,-128]\n\n![](./assets/LAB.png)\n\n## CMY & CMYK\n\n![](./assets/cmyk_1.jpg)\n\n一般用在印刷， 因为人眼看到的物体的颜色是反射光，而不是自发光，一束白光照到物体上，默认反射所有，即白色，人眼实际看到的颜色是用光的颜色减去材料吸收后的颜色,这时涂上黑色的颜料，即吸收了所有白光，所以人眼看到的是黑色。\n利用色料的三原色混色原理，加上黑色油墨，共计四种颜色混合叠加，形成所谓“全彩印刷”。四种标准颜色是：`C：Cyan = 青色`，又称为‘天蓝色’或是‘湛蓝’`M：Magenta = 品红色`，又称为‘洋红色’；`Y：Yellow = 黄色`；`K：blacK=黑色`，虽然有文献解释说这里的K应该是Key Color（定位套版色），但其实是和制版时所用的定位套版观念混淆而有此一说。此处缩写使用最后一个字母K而非开头的B，是为了避免与Blue混淆。CMYK模式是减色模式，相对应的RGB模式是加色模式。\n\n印刷三原色如何得到黑色， 理论配色如下：\n\n> C(100)  +M（100） +Y（100） = 黑色（100，100，100）\n\n可见黑色就是青色、品与黄色之和，但是这三种颜色混成的黑色不够纯，所以印刷学就引进了K(Black)黑色，因为B已经被Blue占用，所以黑色就只好用引文字母黑色的最后一个字母K, 哪么真正印刷的黑色配色如下:\n\n> C(100)  +M（100） +Y（100） + K(100) = 黑色 （100，100，100，100）\n\n或者\n\n> C(0)  +M(0) + Y(0) + K(100) = 黑色(0，0，0，100）\n\n减色模式：前面说的由于物体是吸收了部分光，才呈现出特有的颜色，比如品红的物体，是吸收了它的互补色绿色（看前面的RGB对互补色的描述），也就是白色光减去了绿色才得到的颜色，所以称之为减色模式"}, "/news/others/threshold/threshold.html": {"title": "在线图传调试 LAB 阈值", "content": "---\ntitle: 在线图传调试 LAB 阈值\nkeywords: LAB, 阈值, threshold,\ndesc: Threshold 简介\ndate: 2022-07-12\ntags: 图传 ,在线调试 , 阈值\ncover: ./assets/threshold.png\n---\n\n网上发现一篇不错的文章。这里转载一下，有较大改动 [原文链接](https://bbs.elecfans.com/jishu_2290503_1_1.html)\n\n<!-- more -->\n\n## 前言\n\n根据 官方MaixPy3 和 M2 Dock 的相关说明和官方文档与样例，在大家的帮助下，学习了基础的魔方色块的寻找功能。再此分享给大家。\n\n## 基础科普\n\n### 图传\n\n图传的概念，在无人机中非常常见。\n\n简单来讲，就是把摄像头拍摄的实时视频，又快又好的传递到终端设备上呈现——既要速度，不能卡，卡了没意思；也要质量，清晰度不能低，低了没得玩。而传输速度快，质量高，会占用较多的设备资源，以及需要较大的带宽。所以设计一个上好的图传方案和系统，是很多该行业厂家的重大追求目标之一。\n\n### LAB\n\nLab是一种用数字化的方法来描述人的视觉感应的颜色系统。它是一种设备无关的、基于生理特征的颜色系统。在机器视觉中，Lab的概念会经常提及。\n可以用一张图，来详细描述Lab颜色空间：\n\n![lab](./assets/lab.png)\n\n上示图片，是从人的视觉感应角度来看的。\n- 首先是L：表示亮度，从纯黑到纯白，取值为 [0 -> 100]\n- 然后是a：表示从蓝色到红色的范围，取值为 [-128 -> 127]\n- 最后是b：表示从蓝色到黄色的范围，取值为 [-128 -> 127]\n\n通常，Lab会以范围的形式来表示，也就是Lab阈值，因为因为现场环境的不同，我们看到的颜色，不可能是完完全全的理论纯色，所以给出一定的容错范围；\n例如：[(0, 100, 21, 127, -128, -9)]，分别表示：L-min、L-max、a-min、a-max、b-min、b-max，机器视觉就根据这个范围，来进行颜色判断。\n\n## M2Dock 启用图传\n\n### 相关代码\n\n了解相关概念后，可以开始使用 M2 Dock 图传功能了。\n\n下面就是相关启用图传代码了。其实在[官方文档](https://wiki.sipeed.com/soft/maixpy3/zh/usage/net.html#MJPG-%E5%9B%BE%E4%BC%A0-%E6%80%8E%E4%B9%88%E7%94%A8%EF%BC%9F)都写过\n\n```python\nfrom maix import camera, mjpg, utils, display\n\nqueue = mjpg.Queue(maxsize=8)\nmjpg.MjpgServerThread(\"0.0.0.0\", 18811, mjpg.BytesImageHandlerFactory(q=queue)).start()\n\nwhile True:\n    img = camera.capture()\n    jpg = utils.rgb2jpg(img.convert(\"RGB\").tobytes(), img.width, img.height)\n    queue.put(mjpg.BytesImage(jpg))\n    display.show(img)\n```\n\n上面的代码有三种运行方式：\n1. 在 jupyter 的网页编辑界面，运行上述代码\n2. 可以用 adb shell 或者 ssh 连接到 M2 Dock 后，运行 python ，再输入代码运行\n3. 用 adb shell 或者 ssh 连接到 M2 Dock 后，用vim编辑 tuchuan.py 并保存后，再执行 python ./tuchuan.py 来运行代码\n\n当然对于小白来说，方式1最方便，方式2最麻烦。但是方式3运行效率最好\n\n正确运行上述代码后，接可以开启图传功能了。\n\n### 实际使用\n\n要访问M2 Dock提供的图传功能，可以有几种方式，根据不同情况可以选择不同方式：\n\n#### 使用 OTG 连接运行代码\n\n使用 jyputer 或 adb shell 都可以算作是使用 otg 连接板子\n\n这种情况在成功弹出U盘（即正常连接）后可以在浏览器访问 [http://127.0.0.1:188811](http://127.0.0.1:188811) 来打开图传页面。\n\n![local](./assets/local.png)\n\n原理可查看[这里](https://wiki.sipeed.com/soft/maixpy3/zh/tools/MaixPy3_IDE.html#IDE-%E8%BF%9E%E6%8E%A5%E5%8E%9F%E7%90%86)\n\n#### 非 OTG 运行代码\n\n使用串口或者ssh或者设置成开机运行的代码的话，均可认为是非 OTG 运行代码，这个时候我们要通过IP来访问板子图传网址了，即访问 http://设备ip地址:18811 来查看图传实例\n\n![wireless](./assets/threshold.png)\n\n## Lab阈值实例\n\n前面说过，机器视觉中会利用到Lab，同样的，MaixPy3也提供了很简单的方法来应用Lab阈值。\n\n这里有一个能直接使用的在线工具: [https://wiki.sipeed.com/threshold](https://wiki.sipeed.com/threshold)（加载有点慢） \n\n提供了一个简单的示例，可以自己通过调整下面滑块位置或者输入阈值数值来查看不同的效果\n\n也可以自己手动上传图片然后查看不同阈值所产生的效果\n\n![threshold](./assets/threshold.png)\n\n成功运行上面[图传代码](#m2dock-启用图传)且能够在浏览器中访问到图传画面后，就可以在在线阈值网页中输入正确的图传IP后来直接调整阈值了。上图中右上角有一个图传地址的输入栏，在那里输入正确查看 m2dock 图传的IP后就能够实时查看摄像头录制画面且能够实时调整阈值了。\n\n![threshold_1](./assets/wireless_1.png)\n\n熟练的话可以知道能够调整a值来获取橙色色块阈值，做到下面的效果：\n\n![threshold_2](./assets/wireless_2.png)\n\n上图中可以看到除了红色和橙色之外的颜色都被过滤掉了，其他颜色都成了黑色区域。\n\n可以再调整一下L，来使得比较黑的红色也被排除掉\n\n![threshold_3](./assets/wireless_3.png)\n\n换到其他魔方面调整阈值来说橙色模块更准确点：\n\n![threshold_4](./assets/wireless_4.png)\n\n尽量在多个颜色里面来选出自己需要的颜色，因此像下面这种同一面颜色的不好选出差别\n\n![threshold_5](./assets/wireless_5.png)\n\n所以把魔方打乱后识别效果合适很多。\n\n通过上述说明，可以逐步得到模仿六种颜色的阈值了。\n\n## 实战演示\n\n这里使用一个金字塔魔方来进行展示：\n\n<img src=\"./assets/cube_1.png\" width=50% alt=cube_1><img src=\"./assets/cube_2.png\" width=50% alt=cube_2>\n\n使用支架来将 m2dock 摄像头对准魔方\n\n![snap](./assets/snap.png)\n\n在电脑上进行阈值的调整\n\n![change](./assets/change.png)\n\n最终得到下面的四组值：\n\n```python\n[(0, 100, -128, -23, -128, 127)], #绿色\n[(10, 100, 30, 127, -37, 127)],   #红色\n[(40, 100, -25, 42, 7, 127)],     #黄色\n[(0, 100, -128, 127, -128, -46)], #蓝色\n```\n\n根据官方例程修改后得到下面代码：\n\n```python\nfrom maix import image, display, camera\ncolor = [\n        [(0, 100, -128, -23, -128, 127)], #绿色\n        [(10, 100, 30, 127, -37, 127)], #红色\n        [(40, 100, -25, 42, 7, 127)], #黄色\n        [(0, 100, -128, 127, -128, -46)], #蓝色\n        ]  # 0.5.0 以后蓝色的 lab 阈值，0.4.9 之前为 [(13, 11, -91, 54, 48, -28)]\nfont_color = [ # 边框和文字颜色，暂时都用白色\n    (255,255,255), # 绿色\n    (255,255,255), # 红色\n    (255,255,255), # 黄色\n    (255,255,255)  # 白色\n]\nname_color = ('green', 'red', 'yellow', 'blue')\nwhile True:\n    img = camera.capture()\n    for n in range(0,4):\n        blobs = img.find_blobs(color[n])    #在图片中查找lab阈值内的颜色色块\n        if blobs:\n            for i in blobs:\n                if i[\"w\"]>15 and i[\"h\"]>15:\n                    img.draw_rectangle(i[\"x\"], i[\"y\"], i[\"x\"] + i[\"w\"], i[\"y\"] + i[\"h\"], \n                                       color=font_color[n], thickness=1) #将找到的颜色区域画出来\n                    img.draw_string(i[\"x\"], i[\"y\"], name_color[n], scale = 0.8, \n                              color = font_color[0], thickness = 1) #在红色背景图上写下hello worl\n    display.show(img)\n```\n\n运行上述代码后，识别的效果如下：\n\n<img src=\"./assets/result_1.png\" width=50% alt=result_1><img src=\"./assets/result_2.png\" width=50% alt=result_2>\n\n可以看到已经成功识别出魔方颜色块，且效果还不错。\n\n具体效果视频可以前往 [原链接](https://bbs.elecfans.com/jishu_2290503_1_1.html) 查看"}, "/news/others/python_use.html": {"title": "Python 变量作用域", "content": "---\ntitle: Python 变量作用域\nkeywords: Python, 作用域,\ndesc: Python作用域说明\ndate: 2022-07-13\ntags: Python\n---\n\n这里说明一下 Python 变量作用域，帮助大家更好地使用 Python； [原文链接](https://www.cnblogs.com/Jolly-hu/p/12228320.html)\n\n<!-- more -->\n\n## 作用域简介\n\n**作用域指的是变量的有效范围**。变量并不是在任何位置都可以访问的，访问权限取决于这个变量是在哪里赋值的，也就是在哪个作用域内的。\n\n通常而言，在编程语言中，变量的作用域从代码结构形式来看，有块级、函数、类、模块、包等由小到大的级别。但是在 Python 中，没有块级作用域，也就是类似 if语句块、for语句块、with上下文管理器 等等是不存在作用域概念的，他们等同于普通的语句\n\n```python\nif True:            # if语句块没有作用域\n    x = 1\nprint(x)\n# 1\ndef func():         # 函数有作用域\n    a = 8\nprint(a)\n# Traceback (most recent call last):\n#   File \"<pyshell#3>\", line 1, in <module>\n#     a\n# NameError: name 'a' is not defined\n```\n\n上面的代码可以试着运行一下，然后发现在 if 语句内定义的变量 x，可以被外部访问，而在函数 func() 中定义的变量 a，则无法在外部访问。\n\n通常，函数内部的变量无法被函数外部访问，但内部可以访问；类内部的变量无法被外部访问，但类的内部可以。通俗来讲，就是内部代码可以访问外部变量，而外部代码通常无法访问内部变量。\n\n变量的作用域决定了程序的哪一部分可以访问哪个特定的变量名称。\nPython 的作用域一共有4层，分别是：\n- L （Local） 局部作用域\n- E （Enclosing） 闭包函数外的函数中\n- G （Global） 全局作用域\n- B （Built-in） 内建作用域\n\n```python\nglobal_var = 0  # 全局作用域\ndef outer():\n    out_var = 1  # 闭包函数外的函数中\n    def inner():\n        inner_var = 2  # 局部作用域\n```\n\n前面说的都是变量可以找得到的情况，那如果出现本身作用域没有定义的变量，那该如何寻找呢？\n\nPython 以 L –> E –> G –>B 的规则查找变量，即：在局部找不到，便会去局部外的局部找（例如闭包），再找不到就会去全局找，最后去内建中找。\n\n如果这样还找不到，那就提示变量不存在的错误。例如下面的代码，函数 func 内部并没有定义变量 a，可是 print 函数需要打印 a，那怎么办？\n\n向外部寻找！按照 L –> E –> G –>B 的规则，层层查询，这个例子很快就从外层查找到了 a，并且知道它被赋值为 1，于是就打印了 1。\n\n```python\na = 1\n\ndef func():\n    print(a)\n```\n\n## 全局变量和局部变量\n\n定义在函数内部的变量拥有一个局部作用域，被叫做局部变量，定义在函数外的拥有全局作用域的变量，被称为全局变量。（类、模块等同理）\n\n所谓的局部变量是相对的。局部变量也有可能是更小范围内的变量的外部变量。\n\n局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。\n\n```python\na = 1               # 全局变量\n\ndef func():\n    b = 2           # 局部变量\n    print(a)        # 可访问全局变量a,无法访问它内部的c\n\n    def inner():\n        c = 3       # 更局部的变量\n        print(a)    # 可以访问全局变量a\n        print(b)    # b对于inner函数来说，就是外部变量\n        print(c)\n```\n\n## global 和 nonlocal 关键字\n\n我们先看下面的例子：\n\n```python\ntotal = 0                        # total是一个全局变量\n\ndef plus( arg1, arg2 ):\n    total = arg1 + arg2          # total在这里是局部变量.\n    print(\"函数内局部变量total=  \", total)\n    print(\"函数内的total的内存地址是: \", id(total))\n    return total\n\nplus(10, 20)\nprint(\"函数外部全局变量total= \", total)\nprint(\"函数外的total的内存地址是: \", id(total))\n```\n很明显，函数 plus 内部通过 total = arg1 + arg2 语句，新建了一个局部变量 total，它和外面的全局变量 total 是两码事。而如果我们想要在函数内部修改外面的全局变量 total 呢？使用  global 关键字！ \n\n> global：指定当前变量使用外部的全局变量\n\n```python\nglobal：指定当前变量使用外部的全局变量\n\ntotal = 0                        # total是一个全局变量\n\ndef plus( arg1, arg2 ):\n    global total    # 使用global关键字申明此处的total引用外部的total\n    total = arg1 + arg2\n    print(\"函数内局部变量total=  \", total)\n    print(\"函数内的total的内存地址是: \", id(total))\n    return total\n\nplus(10, 20)\nprint(\"函数外部全局变量total= \", total)\nprint(\"函数外的total的内存地址是:\n```\n所运行结果：\n```python\n函数内局部变量total=   30\n函数内的total的内存地址是:  503494624\n函数外部全局变量total=  30\n函数外的total的内存地址是:  503494624\n```\n我们再来看下面的例子：\n```python\na = 1\nprint(\"函数outer调用之前全局变量a的内存地址： \", id(a))\n\ndef outer():\n    a = 2\n    print(\"函数outer调用之时闭包外部的变量a的内存地址： \", id(a))\n    def inner():\n        a = 3\n        print(\"函数inner调用之后闭包内部变量a的内存地址： \", id(a))\n    inner()\n    print(\"函数inner调用之后，闭包外部的变量a的内存地址： \", id(a))\nouter()\nprint(\"函数outer执行完毕，全局变量a的内存地址： \", id(a))\n```\n如果你将前面的知识点都理解通透了，那么这里应该没什么问题，三个 a 各是各的 a，各自有不同的内存地址，是三个不同的变量。\n\n打印结果也很好的证明了这点：\n```python\n函数outer调用之前全局变量a的内存地址：  493204544\n函数outer调用之时闭包外部的变量a的内存地址：  493204576\n函数inner调用之后闭包内部变量a的内存地址：  493204608\n函数inner调用之后，闭包外部的变量a的内存地址：  493204576\n函数outer执行完毕，全局变量a的内存地址：  493204544\n```\n那么，如果，inner 内部想使用 outer 里面的那个 a，而不是全局变量的那个 a，怎么办？用 global 关键字？先试试看吧：\n```python\na = 1\nprint(\"函数outer调用之前全局变量a的内存地址： \", id(a))\ndef outer():\n    a = 2\n    print(\"函数outer调用之时闭包外部的变量a的内存地址： \", id(a))\n    def inner():\n        global a   # 注意这行\n        a = 3\n        print(\"函数inner调用之后闭包内部变量a的内存地址： \", id(a))\n    inner()\n    print(\"函数inner调用之后，闭包外部的变量a的内存地址： \", id(a))\nouter()\nprint(\"函数outer执行完毕，全局变量a的内存地址： \", id(a))\n```\n运行结果如下，很明显，global使用的是全局变量a。\n```python\n函数outer调用之前全局变量a的内存地址：  494384192\n函数outer调用之时闭包外部的变量a的内存地址：  494384224\n函数inner调用之后闭包内部变量a的内存地址：  494384256\n函数inner调用之后，闭包外部的变量a的内存地址：  494384224\n函数outer执行完毕，全局变量a的内存地址：  494384256\n```\n那怎么办呢？使用 nonlocal 关键字！它可以修改嵌套作用域（enclosing 作用域，外层非全局作用域）中的变量。将 global a 改成 nonlocal a 后运行，代码这里我就不重复贴了，\n\n运行后查看结果，可以看到我们真的引用了 outer 函数的 a 变量。\n```python\n函数outer调用之前全局变量a的内存地址：  497726528\n函数outer调用之时闭包外部的变量a的内存地址：  497726560\n函数inner调用之后闭包内部变量a的内存地址：  497726592\n函数inner调用之后，闭包外部的变量a的内存地址：  497726592\n函数outer执行完毕，全局变量a的内存地址：  497726528\n```\n\n## 面试真题\n不要上机测试，请说出下面代码的运行结果：\n\n```python\na = 10\ndef test():\n    a += 1\n    print(a)\ntest()\n```\n\n很多同学会说，这太简单了！函数内部没有定义 a，那么就去外部找，找到 a=10，于是加 1，打印 11！\n\n我会告诉你，这段代码有语法错误吗？a += 1 相当于 a = a + 1，按照赋值运算符的规则是先计算右边的 a+1。但是，Python 的规则是，如果在函数内部要修改一个变量，那么这个变量需要是内部变量，除非你用 global 声明了它是外部变量。很明显，我们没有在函数内部定义变量 a，所以会弹出局部变量在未定义之前就引用的错误。\n\n## 更多的例子\n\n再来看一些例子（要注意其中的闭包，也就是函数内部封装了函数）：\n\n```python\nname = 'jack'\n\ndef outer():\n    name='tom'\n\n    def inner():\n        name ='mary'\n        print(name)\n\n    inner()\n\nouter()\n```\n上面的题目很简单，因为inner函数本身有name变量，所以打印结果是mary。那么下面这个呢？\n```python\nname ='jack'\n\ndef f1():\n    print(name)\n\ndef f2():\n    name = 'eric'\n    f1()\n\nf2()\n```\n\n这题有点迷惑性，想了半天，应该是‘eric’吧，因为 f2 函数调用的时候，在内部又调用了 f1 函数，f1 自己没有 name 变量，那么就往外找，发现 f2 定义了个 name，于是就打印这个 name。错了！！！结果是‘jack’！\n\n**Python函数的作用域取决于其函数代码块在整体代码中的位置，而不是调用时机的位置**。调用 f1 的时候，会去 f1 函数的定义体查找，对于 f1 函数，它的外部是 name ='jack'，而不是 name = 'eric'。\n\n再看下面的例子，f2 函数返回了 f1 函数：\n\n```python\nname = 'jack'\n\ndef f2():\n    name = 'eric'\n    return f1\n\ndef f1():\n    print(name)\n\nret = f2()\nret()\n```\n\n仔细回想前面的例子，其实这里有异曲同工之妙，所以结果还是‘jack’。"}, "/news/others/Python_call_so.html": {"title": "Python3调用c/cpp的方法", "content": "---\ntitle: Python3调用c/cpp的方法\nkeywords: python, c, cpp,\ndesc: python调用so\ndate: 2022-03-31\ntags: python, c, cpp\n---\n\n<!-- more -->\n\n原文链接：https://blog.csdn.net/springlustre/article/details/101177282\n作者：[springlustre](https://blog.csdn.net/springlustre?type=blog)\n有改动，仅供参考\n\npython中使用 ctypes 模块可以在python中直接调用C/C++。\n首先要将C/C++编译成动态库（.so)，然后python中调用即可。\n\n特别注意在调用C++函数需要在函数声明时，加入前缀 extern \"C\" ，这是因为C++支持函数重载功能，在编译时会改变函数名。在函数声明时，前缀extern \"C\"可以确保按C的方式进行编译。\n\n值得注意的是，一定要有函数输入输出类型的声明，int型不用转换，float和double类型需要进行转换；\nctypes中的变量类型与C中对应如下：\n\n| ctypes数据类型 | C数据类型     |\n| -------------- | ------------- |\n| c_char         | char          |\n| c_short        | short         |\n| c_int          | int           |\n| c_long         | long          |\n| c_float        | float         |\n| c_double       | double        |\n| c_void_p       | void          |\n| c_uint8        | unsigned char |\n\n使用方法：\n- 编写c++代码\n\n```cpp\n#include <iostream>\n#include <string>\n#include <cstdlib>\n#include <vector>\n#include <stdio.h>\n\n\nclass Test{\n    private:\n        double _calculate(int a, double b);\n    public:\n        double calculate(int a, double b, char c[], int * d, double * e, char ** f);\n};\n\ndouble Test::_calculate(int a, double b){\n    double res = a+b;\n    std::cout<<\"res: \"<<res<<std::endl;\n    return res;\n}\n\ndouble Test::calculate(int a, double b, char c[], int * d, double * e, char ** f){\n    std::cout<<\"a: \"<<a<<std::endl;\n    std::cout<<\"b: \"<<b<<std::endl;\n    std::cout<<\"c: \"<<c<<std::endl;\n    std::cout<<\"d: \"<<d[0]<<d[1]<<std::endl;\n    std::cout<<\"e: \"<<e[0]<<e[1]<<std::endl;\n    std::cout<<\"f: \"<<f[0]<<f[1]<<std::endl;\n    return this->_calculate(a, b);\n}\n\n\n// 封装C接口\nextern \"C\"{\n// 创建对象\n    Test* test_new(){\n        return new Test;\n    }\n    double my_calculate(Test* t, int a, double b, char c[], int * d, double * e, char ** f){\n        return t->calculate(a, b,c,d,e,f);\n    }\n}\n\n```\n- 将上面的代码编译成so文件\n\n> g++ -shared -Wl,-soname,test -o test.so -fPIC test.cpp\n\n- 使用python调用so文件\n\n```python\n# -*- coding: utf-8 -*-\nimport ctypes\n# 指定动态链接库\nlib = ctypes.cdll.LoadLibrary('./test.so')\n#需要指定返回值的类型，默认是int\nlib.my_calculate.restype = ctypes.c_double\n\nclass Test(object):\n    def __init__(self):\n        # 动态链接对象\n        self.obj = lib.test_new()\n\n    def calculate(self, a, b,c,d,e,f):\n        res = lib.my_calculate(self.obj, a, b,c,d,e,f)\n        return res\n\n#将python类型转换成c类型，支持int, float,string的变量和数组的转换\ndef convert_type(input):\n    ctypes_map = {int:ctypes.c_int,\n              float:ctypes.c_double,\n              str:ctypes.c_char_p\n              }\n    input_type = type(input)\n    if input_type is list:\n        length = len(input)\n        if length==0:\n            print(\"convert type failed...input is \"+input)\n            return null\n        else:\n            arr = (ctypes_map[type(input[0])] * length)()\n            for i in range(length):\n                arr[i] = bytes(input[i],encoding=\"utf-8\") if (type(input[0]) is str) else input[i]\n            return arr\n    else:\n        if input_type in ctypes_map:\n            return ctypes_map[input_type](bytes(input,encoding=\"utf-8\") if type(input) is str else input)\n        else:\n            print(\"convert type failed...input is \"+input)\n            return null\n\nif __name__ == '__main__':\n    t = Test()\n    A1\t= 123;\n    A2\t= 0.789;\n    A3\t= \"C789\";\n    A4\t= [456,789];\n    A5\t= [0.123,0.456];\n    A6\t= [\"A123\", \"B456\"];\n    print(t.calculate(convert_type(A1), convert_type(A2), convert_type(A3),convert_type(A4),convert_type(A5),convert_type(A6)))\n```"}, "/news/others/maixII_connect_udisk.html": {"title": "MaixII 通过 USB OTG 口连接U盘", "content": "---\ntitle: MaixII 通过 USB OTG 口连接U盘\nkeywords: MaixII, U盘\ndate: 2022-06-14\ntags: MaixII, U盘\n---\n\nMaixII的USB口是用来做device连接电脑跑adb的。\n但是有没有方法可以在不跑adb的时候（总不能天天跑adb吧，再说adb也可以网络跑啊）连接一些USB设备玩玩呢。\n\n<!-- more -->\n\n原文链接：https://bbs.sipeed.com/thread/844 有改动\n\n## 摸索过程\n\nMaixII dock 有两个接口，我们要更改 otg 口因此我们使用 UART口 连接电脑来更改板子设置\n\n### 看看在那里定义了 \n\n在 /etc/init.d/ 文件夹里面可以看到有如下的文件\n\n```bash\nroot@sipeed:# ls /etc/init.d\nS00mpp       S10udev      S40network   S52ntpd      log          rc.preboot\nS01audio     S11dev       S41netparam  adbd         network      rcK\nS01logging   S12usb       S50telnet    cron         rc.final     rcS\nS02app       S20urandom   S51dropbear  fontconfig   rc.modules   sysntpd\n```\n\n注意到里面有一个 `S12usb`\n\n使用 `cat /etc/init.d/S12usb` 查看里面内容后发现有一句 \n\n```bash\notg_role=`cat /sys/devices/platform/soc/usbc0/otg_role`\n```\n\n抱着好奇的心态在设备上跑了这句脚本，结果如下所示：\n\n```bash\nroot@sipeed:~# cat /sys/devices/platform/soc/usbc0/otg_role\nusb_device\n```\n\n### 切换为 USB host\n\n再好奇下看这个 /sys/devices/platform/soc/usbc0 目录中都有啥，结果如下：\n\n```bash\nroot@sipeed:~# ls /sys/devices/platform/soc/usbc0\ndriver           hw_scan_debug    of_node          subsystem        usb_device       usb_null\ndriver_override  modalias         otg_role         uevent           usb_host\n```\n\n重点是里面的：`usb_device` `usb_host` `usb_null`\n\n那直接把 `usb_host` echo 到 `/sys/devices/platform/soc/usbc0/otg_role` 中看看啥效果：\n\n```bash\necho \"usb_host\" > /sys/devices/platform/soc/usbc0/otg_role\n```\n\n然后我们使用 `lsusb` 看看都有啥\n\n```bash\nroot@sipeed:~# lsusb\nBus 001 Device 001: ID 1d6b:0002\nBus 002 Device 001: ID 1d6b:0001\n```\n\n哈，USB控制器出来了。\n\n### 连接USB设备\n\n想着设备内识别SD卡，那U盘应该也差不多。插个U盘试下。\n\n```bash\nroot@sipeed:~# lsusb\nBus 001 Device 001: ID 1d6b:0002\nBus 001 Device 002: ID aaaa:8816\nBus 002 Device 001: ID 1d6b:0001\n```\n\n多出来一个设备，在 /dev 目录下看了下果然多出来一个sda：\n\n```bash\nroot@sipeed:/# ls /dev/sda\nsda   sda1  sda2\n```\n\n挂载 U盘 试试：\n\n出现 `Read-only file system` 的话，重烧是最快的解决方法。\n\n```bash\nroot@sipeed:~# mkdir -p /home/usbdisk\nroot@sipeed:~# mount /dev/sda2 /home/usbdisk/\nroot@sipeed:~# df\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/root               256512     88352    162920  35% /\ntmpfs                    29864        12     29852   0% /tmp\nnone                     29796         0     29796   0% /dev\n/dev/mmcblk0p3            2013         1      2013   0% /mnt/cfg\n/dev/mmcblk0p6         2939292     59664   2863244   2% /mnt/UDISK\n/dev/sda4              7926272    405644   7520628   5% /home/usbdisk\n```\n\n挂载成功。\n\n然后，试了下无线网卡、USB串口啥的，基本都没识别出来，估计是驱动没有编译进去吧。"}, "/news/index.html": {"title": "动态", "content": "---\n\ntitle: 动态\nkeywords: teedoc, 博客生成, 静态博客\ndesc: teedoc 静态博客页面生成\nshow_source: false\ndate: true\n\n---\n<div id=\"blog_list\"></div>"}, "/news/Lichee/RV/run_nonos_program/nonos_run.html": {"title": "在D1上使用裸机程序", "content": "---\ntitle: 在D1上使用裸机程序\nkeywords: RV, D1, 裸机\ndate: 2022-04-29\ntags: MaixPy,MaixPy3\n---\n近日一国内小哥实现了一种在D1上运行裸机程序的方法。让我们一起来看一下吧\n\n<!-- more -->\n\nGithub仓库地址在这里：https://github.com/Ouyancheng/FlatHeadBro\n\n相关使用方法在仓库的 readme 写的很详细了。\n\n大概就是用先编译出一个 启动固件，烧录到SD卡启动板子后可以看到串口有相关的信息打印出来。\n\n接着只需要使用 python 将想要运行的程序通过串口传送到开发板上面即可。"}, "/news/Lichee/RV/D1_RTL8723DS_Drivers/D1_RTL8723DS_Drivers.html": {"title": "D1 LicheeRV Dock 移植RTL8723DS驱动", "content": "---\ntitle: D1 LicheeRV Dock 移植RTL8723DS驱动\nkeywords: D1, RTL8723DS, 驱动\ndesc: RTL8723DS驱动移植\ndate: 2022-04-02\ntags: linux, D1\n---\n\n这里讲解怎样自己添加驱动\n\n<!-- more -->\n\n[原文链接](https://bbs.aw-ol.com/topic/994/d1-licheerv-dock-%E7%A7%BB%E6%A4%8Drtl8723ds%E9%A9%B1%E5%8A%A8)\n\n手动焊接RTL8723DS之后，现在开始移植驱动程序。\n\n先获取源码：https://github.com/lwfinger/rtl8723ds\n\n下载完成后，把驱动文件复制到 tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds 里，没有rtl8723ds文件夹记得新建一个。\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\Makefile，加一行 \nobj-$(CONFIG_RTL8723DS) += rtl8723ds/\n\n![](./assets/rtl8723ds.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\Kconfig，加一行 \nsource \"drivers/net/wireless/rtl8723ds/Kconfig\"\n\n![](./assets/Kconfig.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds\\os_dep\\linux\\os_intfs.c；\n加一行\nMODULE_IMPORT_NS(VFS_internal_I_am_really_a_filesystem_and_am_NOT_a_driver);\n\n![](./assets/os_intfs.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds\\os_dep\\linux\\rtw_cfgvendor.c\n在每一行.policy = VENDOR_CMD_RAW_DATA, 下面加上 .maxattr = 1,\n\n![](./assets/rtw_cfgvendor.png)\n\n修改tina-d1-open\\target\\allwinner\\d1-lichee_rv_dock\\modules.mk，增加以下内容：\n\n![](./assets/modules.png)\n\n（其中的d1-lichee_rv_dock 是我的板级配置，请选择自己的板级配置比如d1-nezha，如下图）\n\n![](./assets/borad_config.png)\n\n进入内核配置，勾选Realtek 8723D SDIO or SPI WiFi为Module（ < M > 不是 < * > ）\n\n```menuconfig\nmake kernel_menuconfig\n\nDevice Drivers ->\n     Network device support -> \n           Wireless LAN -> \n                  <M>   Realtek 8723D SDIO or SPI WiFi\n```\n\n进入Tina配置，勾选相关驱动\n\n```bash\nmake menuconfig\n\nFirmware  ->\n     <*> r8723ds-firmware.............................. RealTek RTL8723DS firmware\n\nKernel modules -> \n     Wireless Drivers  ->\n        <*> kmod-net-rtl8723ds........................... RTL8723DS support (staging)\n```\n\n保存，编译，打包\n\n```bash\nmake -j8\npack\n```\n\n烧录后就能看到\n\n![](./assets/apperance.jpg)"}, "/news/Lichee/RV/D1-ncnn/D1_ncnn.html": {"title": "在全志d1开发板上玩ncnn", "content": "---\ntitle: 在全志d1开发板上玩ncnn\nkeywords: D1, RV, Lichee, ncnn, \ndesc: 在全志d1开发板上玩ncnn\ndate: 2022-03-28\ntags: RV, ncnn\n---\n\n<!-- more -->\n\n转载自知乎用户 [nihui](https://www.zhihu.com/people/nihui-2) [原文链接](https://zhuanlan.zhihu.com/p/386312071)，原文写于 2021-07-03\n\n在全志d1开发板上玩ncnn\n\n**可在不修改本文章内容和banner图前提下，转载本文**\n```\n这是我最后一次优化 risc-v\n这 1.4w 行代码是我最后的倔强\n你们不可能再看见我为这个 d1 写一行代码，不可能\n这 96 个 cpp 文件，我要用到 2030 年\n```\n**首先感谢全志科技公司送了我d1开发板，以及sipeed、rvboards在系统底层技术工作和支持，才有了ncnn AI推理库在risc-v架构上更好的优化 qwqwqwq**\n\n## 0x0 ncnn risc-v 优化情况\n[ncnn](https://github.com/Tencent/ncnn) 是腾讯开源的神经网络推理框架\n- 支持深度学习模型 caffe/mxnet/keras/pytorch(onnx)/darknet/tensorflow(mlir)\n- 跨平台：Windows/Linux/MacOS/Android/iOS/WebAssembly/...\n- 兼容多种 CPU 架构：x86/arm/mips/risc-v/...\n- 支持 GPU 加速：NVIDIA/AMD/Intel/Apple/ARM-Mali/Adreno/...\n- 支持各种常见的模型结构，比如 mobilenet/shufflenet/resnet/LSTM/SSD/yolo...\n- 很强，qq群请移驾 ncnn github 首页README\n_因为据全（某）志（人）说，全志的用户基础都挺一般，可能不知道 ncnn 是什么东西，所以便罗嗦一番..._\n\n从上次发了开箱自拍jpg，到现在一个月了，ncnn risc-v vector 优化情况还算不错，大部分重要的优化都做了，剩下一些会留给社区学生pr，和慢慢变聪明的编译器\n\nncnn risc-v 目前使用 rvv-1.0 intrinisc 编写优化代码，并支持任意 vlen 的配置，面向未来顺便兼容了 d1开发板\n\n- rvv-0.7.1 某些 intrinisc 转换可能有效率问题\n- 有遇到过 0.7.1 intrinisc 行为怪异只能写 C 代码绕过\n- gcc 还比较笨，每行 intrinisc 都会加一条无用的 setvli 指令\n- 因为没法同时兼容 rvv-1.0 和 rvv-0.7.1，便没有写汇编\n- 一些算子，如 hardswish/hardsigmoid/binaryop/eltwise/slice/... 待优化（欢迎pr！！！qaq）\n下面这张表只是最近一周多的情况。如果跟最开始比，柱状图就太高了...\n![](./assets/ncnn/001.jpg)\n\n![](./assets/ncnn/002.jpg)\n\n## 0x1 准备交叉编译工具链\n去平头哥芯片开放社区下载 工具链-900 系列 \nhttps://occ.t-head.cn/community/download?id=3913221581316624384\n比如 riscv64-linux-x86_64-20210512.tar.gz，下载后解压缩，设置环境变量\n```bash\ntar -xf riscv64-linux-x86_64-20210512.tar.gz\nexport RISCV_ROOT_PATH=/home/nihui/osd/riscv64-linux-x86_64-20210512\n```\n## 0x2 下载和编译ncnn\n\n为 d1 架构交叉编译 ncnn\n\n**因为编译器 bug，release 编译会导致运行时非法指令错误，必须使用 relwithdebinfo 编译哦**\n\n- ncnn 已支持直接用 simpleocv 替代 opencv 编译出 examples\n- **不需要配opencv啦！**\n- **不需要配opencv啦！**\n- **不需要配opencv啦！（重要，说了三遍）**\n\n```bash\ngit clone https://github.com/Tencent/ncnn.git\ncd ncnn\nmkdir build-c906\ncd build-c906\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/c906.toolchain.cmake -DCMAKE_BUILD_TYPE=relwithdebinfo -DNCNN_OPENMP=OFF -DNCNN_THREADS=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_RVV=ON -DNCNN_SIMPLEOCV=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j32\n```\n## 0x3 测试benchncnn\n**d1 默认的 TinaLinux 执行 ncnn 程序时会发生非法指令错误，必须使用 Debian 系统哦**\n- vgg16 这类大型模型在内存不足时会发生 segmentation fault，是 d1开发板硬件条件不够，不管即可\n  \n将 `ncnn/build-c906/benchmark/benchncnn` 和 `ncnn/benchmark/*.param` 拷贝到 d1开发板上\n```bash\n./benchncnn 4 1 0 -1 0\n```\n\n## 0x4 测试example\n将 `ncnn/build-c906/examples/nanodet` 和测试图片拷贝到 d1开发板上\n从这里下载 nanodet 模型文件并拷贝到 d1开发板上\nhttps://github.com/nihui/ncnn-assets/tree/master/models\n```bash\n./nanodet test.jpg\n```\n输出检测结果信息，并保存在 image.png\n```\n0 = 0.82324 at 200.04 44.89 198.96 x 253.33\n0 = 0.78271 at 32.98 63.45 178.15 x 232.92\n56 = 0.45923 at 1.46 71.92 90.14 x 117.85\nimshow save image to image.png\nwaitKey stub\n```\n把image.png下载到本地查看，结果已经画在图片上了！d1开发板AI目标检测成功 w\n![](./assets/ncnn/003.jpg)\n\n## 0x5 mips大概也会安排啦，欢迎关注 ncnn github，加qq群交流！\nhttps://github.com/Tencent/ncnn\n\nqq群在 ncnn github 首页 readme 中～"}, "/news/Lichee/RV/D1-ncnn/D1_ncnn_new.html": {"title": "又在全志d1开发板上玩ncnn", "content": "---\ntitle: 又在全志d1开发板上玩ncnn\nkeywords: D1, RV, Lichee, ncnn, \ndesc: 又在全志d1开发板上玩ncnn\ndate: 2022-03-28\ntags: RV, ncnn\n---\n\n\n<!-- more -->\n\n转载自知乎用户 [nihui](https://www.zhihu.com/people/nihui-2) [原文链接](https://zhuanlan.zhihu.com/p/441176926)，原文写于 2021-07-03\n\n又在全志d1开发板上玩ncnn\n\n**可在不修改本文章内容和banner图前提下，转载本文**\n\n## 0x0 工具链变得更好了\n距上次[在全志d1开发板上玩ncnn](./D1_ncnn.html)，已经过去了5个月\n\n在此期间，ncnn收到perfxlab和腾讯犀牛鸟开源人才的学生有关riscv vector的优化\n\n但更重要的是，平头哥收到了社区的反馈，提供了新版工具链\n\n- 支持了 risc-v vector intrinsic v1.0\n- 修复了 release 模式编译 ncnn 时的非法指令问题\nhttps://occ.t-head.cn/community/download?id=3987221940543754240\n\n旧版本工具链的 gcc 比较笨，经常做些负优化，于是试试全新的工具链\n\n## 0x1 配置新的 cmake toolchain\n```bash\n旧\n-march=rv64gcvxtheadc -mabi=lp64d -mtune=c906 -DRVV_SPEC_0_7 -D__riscv_zfh=1 -static\n\n新\n-march=rv64gcv0p7_zfh_xtheadc -mabi=lp64d -mtune=c906 -static\n```\n- arch 参数要用 v0p7，不能用默认的 v，否则会生成非法指令\n- 删除 -DRVV_SPEC_0_7，开启 ncnn 的 rvv-1.0 intrinsic 代码\n- 删除 -D__riscv_zfh=1，arch 参数的 zfh 中已经指代\n\n放在 ncnn/toolchains/c906-v222.toolchain.cmake\n\n## 0x2 工具链修复\n\n因为 rvv-0.7 缺少某些指令支持，遇到一些 rvv-1.0 的代码会生成 unknown op\n```bash\nfneg\nfrec7\nfrsqrt7\n```\n因此要修改下工具链头文件\n\n打开 Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.2.2/lib/gcc/riscv64-unknown-linux-gnu/10.2.0/include/riscv_vector.h\n\n- 找到以下三行\n```h\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rec7)\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rsqrt7)\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, neg)\n```\n- 注释掉\n\n```h\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rec7)\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rsqrt7)\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, neg)\n```\n\n- 找到文件末尾的三个 #endif，添加以下兼容代码，保存\n\n```h\n#endif\n\n#define vfneg_v_f32m1(x, vl) vfsgnjn_vv_f32m1(x, x, vl)\n#define vfneg_v_f32m2(x, vl) vfsgnjn_vv_f32m2(x, x, vl)\n#define vfneg_v_f32m4(x, vl) vfsgnjn_vv_f32m4(x, x, vl)\n#define vfneg_v_f32m8(x, vl) vfsgnjn_vv_f32m8(x, x, vl)\n#define vfneg_v_f16m1(x, vl) vfsgnjn_vv_f16m1(x, x, vl)\n#define vfneg_v_f16m2(x, vl) vfsgnjn_vv_f16m2(x, x, vl)\n#define vfneg_v_f16m4(x, vl) vfsgnjn_vv_f16m4(x, x, vl)\n#define vfneg_v_f16m8(x, vl) vfsgnjn_vv_f16m8(x, x, vl)\n\n#define vfrec7_v_f32m1(x, vl) vfrdiv_vf_f32m1(x, 1.f, vl)\n#define vfrec7_v_f32m2(x, vl) vfrdiv_vf_f32m2(x, 1.f, vl)\n#define vfrec7_v_f32m4(x, vl) vfrdiv_vf_f32m4(x, 1.f, vl)\n#define vfrec7_v_f32m8(x, vl) vfrdiv_vf_f32m8(x, 1.f, vl)\n#define vfrec7_v_f16m1(x, vl) vfrdiv_vf_f16m1(x, 1.f, vl)\n#define vfrec7_v_f16m2(x, vl) vfrdiv_vf_f16m2(x, 1.f, vl)\n#define vfrec7_v_f16m4(x, vl) vfrdiv_vf_f16m4(x, 1.f, vl)\n#define vfrec7_v_f16m8(x, vl) vfrdiv_vf_f16m8(x, 1.f, vl)\n\n#define vfrsqrt7_v_f32m1(x, vl) vfrdiv_vf_f32m1(vfsqrt_v_f32m1(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m2(x, vl) vfrdiv_vf_f32m2(vfsqrt_v_f32m2(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m4(x, vl) vfrdiv_vf_f32m4(vfsqrt_v_f32m4(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m8(x, vl) vfrdiv_vf_f32m8(vfsqrt_v_f32m8(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m1(x, vl) vfrdiv_vf_f16m1(vfsqrt_v_f16m1(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m2(x, vl) vfrdiv_vf_f16m2(vfsqrt_v_f16m2(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m4(x, vl) vfrdiv_vf_f16m4(vfsqrt_v_f16m4(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m8(x, vl) vfrdiv_vf_f16m8(vfsqrt_v_f16m8(x, vl), 1.f, vl)\n\n#endif\n#endif\n```\n\n## 下载和编译ncnn\n这次可以用 release 编译啦！\n```bash\ngit clone https://github.com/Tencent/ncnn.git\ncd ncnn\nmkdir build-c906\ncd build-c906\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/c906-v222.toolchain.cmake -DCMAKE_BUILD_TYPE=release -DNCNN_OPENMP=OFF -DNCNN_THREADS=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_RVV=ON -DNCNN_SIMPLEOCV=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j32\n```\n\n## 新旧工具链的性能测试对比\n![](./assets/ncnn_new/ncnn_new_001.jpg)\n\n![](./assets/ncnn_new/ncnn_new_002.jpg)\n\n## 0x5 欢迎关注 ncnn github，加qq群交流！\nhttps://github.com/Tencent/ncnn\nqq群在 ncnn github 首页 readme 中～"}, "/news/MaixPy3/v831_Distance/v831_Distance.html": {"title": "V831完美的单目测距", "content": "---\ntitle: V831完美的单目测距\nkeywords: V831, 单目, 测距\ndate: 2022-03-28\ndesc: V831完美的单目测距\ntags: V83x,单目测距\n---\n\n<!-- more -->\n\n作者[我与nano](https://qichenxi.blog.csdn.net/?type=blog)，[原文链接](https://blog.csdn.net/qq_51963216/article/details/123745657)\n\n## 前言\n\n经过一下午的努力，最终终于实现了完美的单目测距，网上教的都是opencv怎么测算距离，人家有函数唉，入手了V831，做了人脸识别，同时进行了测距，K210通用。废话不多说上图。\n\n![单目测距](./assets/distance_measure.png)\n![摄像头距离](./assets/Camera_length.png)\n它那个镜头其实还要在靠近里面一点，距离应该是28.4到28.5之间。测得真的特别准。\n\n## 单目测距的原理\n![principle](./assets/principle.png)\n\n小孔成像。很简单，用的是小孔成像，原理大家都知道。该怎么做呢。\n我们需要以下几个参数：\n1、相机焦距\n2、物体宽度\n3、一个常数\n\n## 参数计算\n\n### 相机焦距\n假设我们有一个宽度为 W 的目标。然后我们将这个目标放在距离我们的相机为 D 的位置。我们用相机对物体进行拍照并且测量物体的像素宽度 P 。这样我们就得出了相机焦距的公式：\n\nF = (P x D) / W\n\n举个例子，假设我在离相机距离 D = 28cm的地方放一张 待识别图片（W = 13)并且拍下一张照片。我测量出照片的像素宽度为 P = 53 像素\n\n![](./assets/length_calculate.png)\n\n因此我的焦距 F 是：\n\nF = (53*28) / 13 = 116\n\n有人会问像素怎么获得呢，直接看代码吧\n```python\n img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n            img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n            img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\n            img.draw_string(0,30, \"x=\"+str(((box[0]+box[3])/2)-35), color= font_color)\n            img.draw_string(70,30, \"y=\"+str((box[1]+box[2])/2), color= font_color)\n\n            Lm = (box[1]+box[3])/2\n            length = K*13/Lm\n            img.draw_string(0,60 , \"Z=\"+str(length), color= font_color)\n\n```\n你识别到一个物体，然后给它画框，用一个列表表示出来四个点\nLm=（box[1]+box[3]）/2 这个就是像素值\n\n### 测距\n继续将相机移动，靠近或者离远物体或者目标时，可以用相似三角形计算出物体离相机的距离：\nL= (W x F) / P\n假设我将相机移到距离目标 28cm 的地方识别物体。通过自动的图形处理我可以获得图片的像素为 53像素。将这个代入公式，得：\nL= (13 x 116) / 53 = 28\n这样我们就精准的算出了距离。\n\n附上代码\n```python\nfrom maix import camera, image, display\nimport serial\nser = serial.Serial(\"/dev/ttyS1\",115200)    # 连接串口\nK=116\nclass Face_recognize :\n    score_threshold = 70                            #识别分数阈值\n    input_size = (224, 224, 3)                      #输入图片尺寸\n    input_size_fe = (128, 128, 3)                   #输入人脸数据\n    feature_len = 256                               #人脸数据宽度\n    steps = [8, 16, 32]                             #\n    channel_num = 0                                 #通道数量\n    users = []                                      #初始化用户列表\n    threshold = 0.5                                         #人脸阈值\n    nms = 0.3\n    max_face_num = 3                                        #输出的画面中的人脸\n    def __init__(self):\n        from maix import nn, camera, image, display\n        from maix.nn.app.face import FaceRecognize\n        for i in range(len(self.steps)):\n            self.channel_num += self.input_size[1] / self.steps[i] * (self.input_size[0] / self.steps[i]) * 2\n        self.channel_num = int(self.channel_num)     #统计通道数量\nglobal face_recognizer\nface_recognizer = Face_recognize()\nwhile True:\n    img = camera.capture()                       #获取224*224*3的图像数据\n    AI_img = img.copy().resize(224, 224)\n    faces = face_recognizer.face_recognizer.get_faces(AI_img.tobytes(),False)           #提取人脸特征信息\n\n    if faces:\n        for prob, box, landmarks, feature in faces:\n            disp_str = \"face\"\n            bg_color = (0, 255, 0)\n            font_color=(255, 0, 0)\n            box,points = face_recognizer.map_face(box,landmarks)\n            font_wh = image.get_string_size(disp_str)\n            for p in points:\n                img.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)\n            img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n            img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n            img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\n            img.draw_string(0,30, \"x=\"+str(((box[0]+box[3])/2-28)), color= font_color)\n            img.draw_string(70,30, \"y=\"+str((box[1]+box[2])/2-20), color= font_color)\n            x=(box[0]+box[3])/2-28\n            y=(box[1]+box[2])/2\n            Lm = (box[1]+box[3])/2\n            length = K*13/Lm\n            img.draw_string(0,60 , \"Z=\"+str(round(length)), color= font_color)\n           \n    display.show(img)\n\n```\n\n## 总结\n\n**主要原理就是小孔成像**"}, "/news/MaixPy3/camera_resize/camera_resize.html": {"title": "MaixPy3 Image.resize 的效果", "content": "---\ntitle: MaixPy3 Image.resize 的效果\nkeywords: MaixPy3,\ndesc: 这里展示一下 MaixPy3 Image.resize 的效果\ndate: 2022-07-01\ntags: MaixPy3, resize, teedoc\n---\n\n点击下载对应的 ipynb 文件后导入到 jupyter 照样能阅读 [源文件](https://dl.sipeed.com/fileList/others/wiki_news/maixpy3_resize/camera_resize.ipynb)\n\n<!-- more -->\n\n## 先写一些 jupyter 用法\n\n- 每一个框框都被称之为单元格\n\n- 单元格左方会有 蓝色 或者 绿色 两种颜色。绿色表示编辑模式；蓝色表示命令模式。\n    - 通用：\n        - Shift+ Enter ：运行单元格，且以命令模式切换到下一个单元格\n        - Ctrl + Enter ：运行单元格，且进入命令模式\n    - 编辑模式中：\n        - Esc       ：进入命令模式\n    - 命令模式中：\n        - **h    :打开帮助**\n        - Enter :进入编辑模式\n        - x    :剪切单元格\n        - c    :复制单元格\n        - v    :粘贴单元格\n        - dd   :删除整个单元格\n        - ii   :终止运行 \n\n## 开始演示效果\n\n### 试一下推流\n\n```python\nimport os\nos.system(\"killall python3\")             #先杀一次预防 camera is busy\nfrom maix import camera, display, image #引入python模块包\nwhile True:\n    img = camera.capture()    #从摄像头中获取一张图像\n    display.show(img)         #将图像显示出来\n    \n    #因为这是死循环，所以按一下 Esc 进入编辑模式然后 ii 终止一下代码\n```\n\n效果如下（视频被截成图片了）：\n\n![推流](./assets/forever_show.jpeg)\n\n### 设置一下图像分辨率\n\n下面就直接捕获原图和使用 image.resize() 放一起对比一下\n\n#### 试试 240*240 的显示效果\n\n``` python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(240, 240))   #设置获取图像分辨率\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"240x240.jpg\")\n```\n\n![240*240](./assets/240_240.jpeg)\n\n#### 240\\*240图片resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(240, 240))\nimg = camera.capture().resize(224, 224)\ndisplay.show(img)\n```\n\n![240*240->224*224](./assets/240_240_224_224.png)\n\n#### 试试 320*240 的显示效果\n\n```python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(320, 240))\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"320x240.jpg\")\n```\n\n![320*240](./assets/320_240.jpeg)\n\n#### 320\\*240图像resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(320, 240))\nimg = camera.capture().resize(224, 244)\ndisplay.show(img)\n```\n\n![320*240->224*224](./assets/320_240_224_224.png)\n\n#### 再试试 320*180 显示效果\n\n```python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(320, 180))    #设置摄像头分辨率\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"320x180.jpg\")\n```\n\n![320*180](./assets/320_180.jpeg)\n\n#### 320\\*180图像resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(320, 180))\nimg = camera.capture().resize(224, 244)\ndisplay.show(img)\n```\n\n![320*180->224*224](./assets/320_180_224_224.png)"}, "/news/MaixPy3/run_lvgl/run_lvgl.html": {"title": "V831 如何使用 libmaix SDK C++ 开发", "content": "---\ntitle: V831 如何使用 libmaix SDK C++ 开发\nkeywords: V831, 交叉编译, lvgl\ndate: 2022-05-19\ndesc: V831 如何使用 libmaix SDK C++ 开发 交叉编译\ntags: V831，交叉编译\n---\n\n<!-- more -->\n\n作者[Song](QQ群友)，原文文件 [点我下载](https://dl.sipeed.com/fileList/others/wiki_news/v831_lvgl_news/220519UbuntuForV831%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.pptx)\n\n下面为重新整理的部分内容\n\n## 准备环境\n\n### 准备 linux\n\n一般在 linux 系统下开发比 windows 系统里问题少。因此首先自己整一个 linux 环境先。\n\n- 物理机和虚拟机都可以\n\n下面使用 Ubuntu18.04 作为实验系统。\n\n首先安装相关依赖 `sudo apt install build-essential cmake python3 sshpass git`\n\n然后确保一下cmake版本>=3.9\n![](./assets/cmake-version.png)\n\n### 配置交叉编译链\n\n先下载这个[点我跳转](https://dl.sipeed.com/shareURL/MaixII/MaixII-Dock/SDK/Toolchain)\n![](./assets/toolchain.png)\n\n接着再对应的下载目录执行下面的命令将工具链解压到 /opt 目录下\n`sudo tar -Jxvf toolchain-sunxi-musl-pack-2021-01-09.tar.xz -C /opt`\n\n### 获取libmaix源码\n\n新建一个文件夹后打开文件夹。\n在对应文件夹的终端使用下面命令来获取源码\n`git clone https://github.com/sipeed/libmaix --recursive`\n\n一定要确保全部下载完成，否则后面会因为找不到文件编译出错。\n\n### 开始编译\n\n#### 尝试编译helloworld\n\n先进入 libmaix 源码目录的 helloworld 文件夹里\n```bash\ncd libmaix/examples/hello-world\n```\n\n根据 CPU 架构选择工具链和前缀\n```bash\npython3 project.py --toolchain /opt/toolchain-sunxi-musl/toolchain/bin --toolchain-prefix arm-openwrt-linux-muslgnueabi- config\n```\n![](./assets/helloworld.png)\n\n接着就可以编译 helloworld 了。在上面的命令成功执行后接着执行下面的命令\n```bash\npython3 project.py menuconfig\n```\n\n若出现以下画面，则说明下载内容完整。若报错先尝试使用sudo执行，否则需重新下载解压\n![](./assets/helloworld-menuconfig.png)\n\n如第三项选择了Enable 3rd party component，则在编译时时间就会较长，因为会编译所有勾选的第三方组件。编译过程中可能会报一些warning，但最终出现下图画面则说明编译过程无误\n<img src=\"./assets/enable-3rd-party-component.png\">\n<img src=\"./assets/finish-helloworld.png\">\n\n前面的都顺利结束后在当前目录下会有一个 dist 文件夹。里面的 helloworld 文件就是 v831 的可执行程序。\n\n我们可以用 ssh 或者使用 v831在电脑上显示的U盘 把 helloworld 可执行文件传输到板子上\n\n接着在对应的目录下直接执行就可以看到结果了\n![](./assets/run-helloworld.png)\n\n#### lvgl编译与测试\n\n在 libmaix 源码的路径下的 /examples/mpp_v83x_vivo 执行下面命令后按照图示配置\n```bash\npython3 project.py menuconfig\n```\n<html>\n<div class=\"imbox\">\n    <img src=\"./assets/lvgl-1.png\" >\n    <img src=\"./assets/lvgl-2.png\" >\n</div>\n</html>\n\n检查选项是否如以上配置所示，确认无误后在命令行执行\n```bash\npython3 project.py build\n```\n- 注：若同时勾选所有组件，则可能会发生重复定义函数的报错导致编译失败\n\n若出现如图所示情况，则说明编译成功\n<img src=\"./assets/lvgl-3.png\">\n\n在板子上运行\n<html>\n<div class=\"imbox\">\n    <img src=\"./assets/lvgl-4.png\" height=300>\n    <img src=\"./assets/lvgl-5.png\" height=300>\n<style>\n.imbox{\n     display:flex;\n     flex-direction: row;\n     }\n</style>\n</div>\n</html>"}, "/news/MaixPy3/maixpy3_easyuse/maixpy3_easyuse.html": {"title": "MaixPy3 源码怎么样", "content": "---\ntitle: MaixPy3 源码怎么样\nkeywords: V831, Maixpy3\ndate: 2022-04-29\ntags: MaixPy3,QQ\n---\n\n这里只使用一张图来说明一下相关的结论\n\n<!-- more -->\n\n![](./assets/pic.jpg)"}, "/news/MaixPy3/key_face_recognize.html": {"title": "V831的人脸识别", "content": "---\ntitle: V831的人脸识别\nkeywords: MaixII-Dock, MaixPy3, 人脸识别, V831\ndesc: V831的人脸识别\ndate: 2022-03-15\ntags: MaixII-Dock, MaixPy3\n---\n\n在文档中看到 V831 可以用来实现人脸识别，于是就将按键也添加到人脸识别中。\n\n<!-- more -->\n\n实现一个可以通过按键进行控制的人脸识别，进行人脸信息的添加和删除控制\n## 源码\n\n```python\nfrom maix import nn, camera, image, display\nfrom maix.nn.app.face import FaceRecognize\nimport time\nfrom evdev import InputDevice\nfrom select import select\n\n\nscore_threshold = 70                            #识别分数阈值\ninput_size = (224, 224, 3)                      #输入图片尺寸\ninput_size_fe = (128, 128, 3)                   #输入人脸数据\nfeature_len = 256                               #人脸数据宽度\nsteps = [8, 16, 32]                             #\nchannel_num = 0                                 #通道数量\nusers = []                                      #初始化用户列表\nnames = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]  #人脸标签定义\nmodel = {                                                                                                                                   \n    \"param\": \"/home/model/face_recognize/model_int8.param\",\n    \"bin\": \"/home/model/face_recognize/model_int8.bin\"\n}\nmodel_fe = {\n    \"param\": \"/home/model/face_recognize/fe_res18_117.param\",\n    \"bin\": \"/home/model/face_recognize/fe_res18_117.bin\"\n}\n\n\nfor i in range(len(steps)):\n    channel_num += input_size[1] / steps[i] * (input_size[0] / steps[i]) * 2\nchannel_num = int(channel_num)     #统计通道数量\noptions = {                             #准备人脸输出参数\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": input_size\n    },\n    \"outputs\": {\n        \"output0\": (1, 4, channel_num) ,\n        \"431\": (1, 2, channel_num) ,\n        \"output2\": (1, 10, channel_num) \n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\noptions_fe = {                             #准备特征提取参数\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"inputs_blob\": input_size_fe\n    },\n    \"outputs\": {\n        \"FC_blob\": (1, 1, feature_len)\n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\nkeys = InputDevice('/dev/input/event0')\n\nthreshold = 0.5                                         #人脸阈值\nnms = 0.3                                               \nmax_face_num = 1                                        #输出的画面中的人脸的最大个数\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\nprint(\"-- load model:\", model_fe)\nm_fe = nn.load(model_fe, opt=options_fe)\nprint(\"-- load ok\")\nface_recognizer = FaceRecognize(m, m_fe, feature_len, input_size, threshold, nms, max_face_num)\n\ndef get_key():                                      #按键检测函数\n    r,w,x = select([keys], [], [],0)\n    if r:\n        for event in keys.read(): \n            if event.value == 1 and event.code == 0x02:     # 右键\n                return 1\n            elif event.value == 1 and event.code == 0x03:   # 左键\n                return 2\n            elif event.value == 2 and event.code == 0x03:   # 左键连按\n                return 3\n    return 0\n\ndef map_face(box,points):                           #将224*224空间的位置转换到240*240或320*240空间内\n    # print(box,points)\n    if display.width() == display.height():\n        def tran(x):\n            return int(x/224*display.width())\n        box = list(map(tran, box))\n        def tran_p(p):\n            return list(map(tran, p))\n        points = list(map(tran_p, points))\n    else:\n        # 168x224(320x240) > 224x224(240x240) > 320x240\n        s = (224*display.height()/display.width()) # 168x224\n        w, h, c = display.width()/224, display.height()/224, 224/s\n        t, d = c*h, (224 - s) // 2 # d = 224 - s // 2 == 28\n        box[0], box[1], box[2], box[3] = int(box[0]*w), int((box[1]-28)*t), int(box[2]*w), int((box[3])*t)\n        def tran_p(p):\n            return [int(p[0]*w), int((p[1]-d)*t)] # 224 - 168 / 2 = 28 so 168 / (old_h - 28) = 240 / new_h\n        points = list(map(tran_p, points))\n    # print(box,points)\n    return box,points\n\ndef darw_info(draw, box, points, disp_str, bg_color=(255, 0, 0), font_color=(255, 255, 255)):    #画框函数\n    box,points = map_face(box,points)\n    font_wh = image.get_string_size(disp_str)\n    for p in points:\n        draw.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)\n    draw.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n    draw.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n    draw.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\ndef recognize(feature):                                                                   #进行人脸匹配\n    def _compare(user):                                                         #定义映射函数\n        return face_recognizer.compare(user, feature)                      #推测匹配分数 score相关分数\n    face_score_l = list(map(_compare,users))                               #映射特征数据在记录中的比对分数\n    return max(enumerate(face_score_l), key=lambda x: x[-1])                #提取出人脸分数最大值和最大值所在的位置\n\ndef run():\n    img = camera.capture()                       #获取224*224*3的图像数据\n    AI_img = img.copy().resize(224, 224)\n    if not img:\n        time.sleep(0.02)\n        return\n    faces = face_recognizer.get_faces(AI_img.tobytes(),False)           #提取人脸特征信息\n    if faces:\n        for prob, box, landmarks, feature in faces:\n            key_val = get_key()\n            if key_val == 1:                                # 右键添加人脸记录\n                if len(users) < len(names):\n                    print(\"add user:\", len(users))\n                    users.append(feature)\n                else:\n                    print(\"user full\")\n            elif key_val == 2:                              # 左键删除人脸记录\n                if len(users) > 0:\n                    print(\"remove user:\", names[len(users) - 1])\n                    users.pop()\n                else:\n                    print(\"user empty\")\n\n            if len(users):                             #判断是否记录人脸\n                maxIndex = recognize(feature)\n\n                if maxIndex[1] > score_threshold:                                      #判断人脸识别阈值,当分数大于阈值时认为是同一张脸,当分数小于阈值时认为是相似脸\n                    darw_info(img, box, landmarks, \"{}:{:.2f}\".format(names[maxIndex[0]], maxIndex[1]), font_color=(0, 0, 255, 255), bg_color=(0, 255, 0, 255))\n                    print(\"user: {}, score: {:.2f}\".format(names[maxIndex[0]], maxIndex[1]))\n                else:\n                    darw_info(img, box, landmarks, \"{}:{:.2f}\".format(names[maxIndex[0]], maxIndex[1]), font_color=(255, 255, 255, 255), bg_color=(255, 0, 0, 255))\n                    print(\"maybe user: {}, score: {:.2f}\".format(names[maxIndex[0]], maxIndex[1]))\n            else:                                           #没有记录脸\n                darw_info(img, box, landmarks, \"error face\", font_color=(255, 255, 255, 255), bg_color=(255, 0, 0, 255))\n\n\n    display.show(img)\n\n\n\nif __name__ == \"__main__\":\n    import signal\n    def handle_signal_z(signum,frame):\n        print(\"APP OVER\")\n        exit(0)\n    signal.signal(signal.SIGINT,handle_signal_z)\n    while True:\n        run()\n\n```"}, "/news/MaixPy3/difference.html": {"title": "MaixPy 与 MaixPy3 的区别", "content": "---\ntitle: MaixPy 与 MaixPy3 的区别\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: MaixPy 与 MaixPy3 的区别\ndate: 2022-03-07\ntags: MaixPy,MaixPy3\n---\n\n<!-- more -->\n\n## 区别是？\n\n因为使用 MaixPy 的同学可能有两类人群，一类是从 MicroPython 一路使用过来的，另一类是从 Python3 过来的，所以针对两边的差异，分别做一下说明。\n\n可以这样理解，它们都是专门为 AIoT 提供的 Python 开发环境，提供了各种各样的模块。\n\n- MaixPy 指的是基于 MicroPython 的环境制作的。\n\n- MaixPy3 指的是基于 Linux Python3 的环境制作的。\n\n> 前者是基于 MCU 无系统的，后者是基于 Linux 系统。\n\n除了基本的 Python3 语法一致，在提供的模块方面的存在着不小的差异。\n\n### Python3 与 MicroPython 的区别\n\n大多数时候，Python 的发展以 Python3 为主，以下列出一些与 Python3 的差异化信息。\n\n- MicroPython 和 Python3 在 Python 语法上保持高度的一致性，常用的标准语法命令都已经支持。\n\n- MicroPython 虽然只实现了 Python3 的标准库和容器库的一些部分，常见容器库有同类功能，但不同名的模块，但大多算法类的 Python 逻辑代码是可以拿来即用的。\n\n- MicroPython 兼容实现的 Python3 的异常机制、没有实现元类（metaclass）机制，独立的 GC 机制。\n\n- 在许当不同的硬件微芯片（最低在 nRF51）的移植上， MicroPython 代码接口缺乏一致性，呈现碎片化。\n\n- MicroPython 编译（mpy-corss）后得到的是 mpy ，而不是 Python3 的 pyc 文件。\n\n- MicroPython 在移植 Python3 代码时，经常缺少各种方法，所以要习惯寻找同类接口，而它们的使用方法除了看文档外就只能看源码。\n\n### 总结\n\n- MaixPy 相比 MaixPy3 功能要更简单（简陋）。\n- MaixPy 和 MaixPy3 的开发工具不同。\n- MaixPy 标准库（MicroPython）相比 MaixPy3 有一定的不足。\n- MaixPy 的外设驱动模块具体函数存在差异。\n- 不同的芯片执行效率有差异，MaixPy 和 MaixPy3 的有着不同的内存与性能消耗。\n\n> 如有更多欢迎补充。"}, "/news/MaixPy/reload_python_module.html": {"title": "关于 MicroPython import 指定 flash 或 sd 分区的代码与重载 Python 模块的方法", "content": "---\ntitle: 关于 MicroPython import 指定 flash 或 sd 分区的代码与重载 Python 模块的方法\nkeywords: MicroPython\ndate: 2022-06-09\ntags: MicroPython, reload\n---\n\n如果在 maixpy (micropython) 上同时存在 flash 和 sd 等多个分区挂载 / 目录下，且均存在 boot.py 文件，如何加载指定分区下的 boot.py 模块代码呢？\n\n<!-- more -->\n\n[原文链接](https://www.cnblogs.com/juwan/p/14517375.html) https://www.cnblogs.com/juwan/p/14517375.html\n\n`import boot` 时取决于 os 的 vfs (虚拟文件系统) 对象，它会根据 os.getcwd() 和 os.chdir('/sd') 决定代码寻找的位置（/sd 分区路径），如果是某目录下的代码，则可以使用类似 import test.boot 的结构来查找并 import 它。\n\n示例：\n\n```python\n>>> os.chdir('/flash')\n>>> import boot\nflash: 2942\n>>> os.getcwd()\n'/flash'\n>>> \n```\n\n拓展来讲，如何重载 import boot 后的 boot 模块，管理 sys.modules 模块就行，如下示意。\n\n```python\n>>> import sys\n>>> import boot\n2433\n>>> import boot\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>> sys.modules.pop('boot')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: boot\n>>> os.chdir('/flash')\n>>> import boot\nflash: 2479\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>> os.chdir('/sd')\n>>> import boot\n2488\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>>\n```\n\n即先使用 sys.modules.pop('boot') 后再重新 import 目标 boot 就行"}, "/news/MaixPy/kmodel_datastruct.html": {"title": "K210 kmodel 模型储存数据结构", "content": "---\ntitle: K210 kmodel 模型储存数据结构\nkeywords: K210, kmodel\ndate: 2022-06-09\ntags: K210, kmodel\n---\n\nK210 kmodel 模型储存结构\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/307 有改动\n\n## K210 kmodel 简介\n\nV3 由 nncase v0.1.0 RC5 转换而来\nV4 由 nncase v0.2.0 从 tflite 转换而来\n\nV4相比于V3, 支持了更多算子, 但是运算速度更慢, 部分算子使用了CPU运算, K210侧也使用了C++编写, 部分库会拉低速度, 如果你需要移植或者优化可以注意这一点\n\n## kmodel V3 数据结构\n| 头                  | 输出层信息                        | 各层的头                                 | 各个层数据内容 |\n| ------------------- | --------------------------------- | ---------------------------------------- | -------------- |\n| kpu_kmodel_header_t | kpu_model_output_t * output_count | kpu_model_layer_header_t * layers_length | layers_body    |\n\n这里有个注意点, kpu_kmodel_header_t 没有8字节对齐, 所以在第一层的数据实际是保存在8字节对齐处, 比如前面所有header 长度为 228 字节, 那么 第一层数据中, 头 kpu_model_conv_layer_argument_t 占用24个字节, 228+24 不是8的整数倍, 所以层数据保存在 228+24+4 处, 所以在 kpu_model_conv_layer_argument_t 中用了 layer_offset 这个来表示层数据相对于模型起始地址的偏移\n\n```c\ntypedef struct\n{\n    uint32_t version;     // 固定  0x00000003, 0x03在低地址\n    uint32_t flags;       // 最低位 为1, 表示8bit模式\n    uint32_t arch;\n    uint32_t layers_length;\n    uint32_t max_start_address;\n    uint32_t main_mem_usage;\n    uint32_t output_count;\n} kpu_kmodel_header_t;\ntypedef struct\n{\n    uint32_t address;\n    uint32_t size;\n} kpu_model_output_t;\ntypedef struct\n{\n    uint32_t type;\n    uint32_t body_size;\n} kpu_model_layer_header_t;\n```\n\n## kmodel V4 数据结构\n| 头                    | 输入                     | 输入形状                    | 输出                      | 常量          | 各层的头                        | 各个层数据内容 |\n| --------------------- | ------------------------ | --------------------------- | ------------------------- | ------------- | ------------------------------- | -------------- |\n| struct modelv4_header | memory_range\\*hdr.inputs | runtime_shape_t\\*hdr.inputs | memory_range\\*hdr.outputs | hdr.constants | hdr.nodes \\* struct node_header | nodes content  |\n\n```c\nstruct modelv4_header\n{\n    uint32_t identifier; // 固定为 KMDL, L在低地址\n    uint32_t version;    // 固定为 0x00000004, 0x04 在低位\n    uint32_t flags;\n    uint32_t target;     // CPU: 0, K210: 1\n    uint32_t constants;  // 多少个 uint_t 类型的常量\n    uint32_t main_mem;   // 主内存, 用于AI, 运行时会先把输入的数据拷贝到这里\n    uint32_t nodes;\n    uint32_t inputs;     // input size\n    uint32_t outputs;    // output size\n    uint32_t reserved0;\n};\nstruct node_header\n{\n    uint32_t opcode;\n    uint32_t size;\n};\nstruct memory_range\n{\n    memory_type_t memory_type;\n    datatype_t datatype;\n    uint32_t start;\n    uint32_t size;\n};  // 16 Bytes\ntypedef enum _datatype\n{\n    dt_float32,\n    dt_uint8\n} datatype_t;\ntypedef enum _memory_type\n{\n    mem_const,\n    mem_main,\n    mem_k210_kpu\n} memory_type_t;\nusing runtime_shape_t = std::array<int, 4>;\n```"}, "/news/MaixPy/K210_kflash_ISP_download_progress.html": {"title": "K210 kflash ISP 下载程序流程", "content": "---\ntitle: K210 kflash ISP 下载程序流程\nkeywords: K210, kflash, ISP\ndate: 2022-06-09\ntags: K210, kflash\n---\n\nK210详细的的程序下载流程，包括芯片侧和kflash侧\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/312 有改动\n\n## 术语\n\n- ISP: In System Programming, 在系统编程\n\n## kflash 下载流程\n\n> 带 || 开头的是芯片侧的操作\n\n- || 芯片拷贝 `boot rom` (特定的硬件，一次性写入)中的 boot 程序到内存末尾1()，并调用运行这个 boot 程序\n\n- || boot 程序运行后， 从 otp 读取信息来判断是否需要从 top 中读取新的 boot，因为 boot rom 区域是一次性写入的，如果 boot 写出了 bug，可以用 otp 中写入新的 boot 来挽救，相当于芯片出厂有两次写入 boot 的机会。而事实是 k210 确实用上了这个功能\n\n- || 如果需要使用 otp 中的新 boot，则读取到内存末尾，但是需要在现在正在运行的 boot 前面，比如之前的末尾1是倒数16k，那么这个 otp 的 boot 就需要写到之前,比如倒数32k到倒数16k位置，然后跳转执行这个新的 boot\n\n- || boot 程序判断 boot 引脚是否被拉低，没被拉低则进入正常启动模式，读取整个固件到内存，然后启动，否则进入ISP模式\n\n- 上位机打开串口\n\n- 上位机通过串口的 dtr rts 来设置 boot 和 reset 引脚，保持拉低 boot引脚，然后拉低reset引脚再拉高reset引脚， 即让芯片重启的时候保持boot引脚为低电平\n\n- || 芯片boot程序检测，如果boot引脚被拉低了，则进入 ISP 模式(输入boot程序的一部分)\n\n- 上位机向芯片发送握手信号(b'\\xc0\\xc2\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0')，然后等待响应信号\n\n- 默认芯片是115200波特率，如果需要更高波特率，发送修改波特率命令给芯片，芯片程序(boot)修改通信波特率\n\n- 向芯片发送写 ramrum 程序的命令，并将需要 RAMRUM 的程序或者新的 ISP 程序发过去，写到芯片内存的起始地址\n\n注意到，这里没有直接发送要写入到flash的程序过去让boot里面的ISP程序去写入到Flash，而是重新发送了一份ISP程序过去，后面会运行这份新的ISP程序来从串口获取固件并烧录到flash，这可以说是为了更灵活，可以自定义ISP程序，而且ISP程序的大小只要小于前面的 boot 和 otp 的新boot的大小之和就行，boot里面做这些事可能面临boot程序过大或者有bug后期更新的问题。但是代价就是每次下载程序都要花费几秒钟下载新的ISP程序，这样用户每次下载的时间会变长，所以最好的肯定是一次性把boot程序写完美，没有bug。其实也可以在boot的ISP程序中加入写如程序到flash的命令,这样是否使用新的ISP就可选\n\n- 向芯片发送启动 ramrum 命令\n\n- 如果是想在ram中运行程序，到这一步为止即可，程序已经在ram中运行了，否则往下一步\n\n- 接下来就是运行发送过去的新ISP程序了\n\n- 上位机和ISP程序握手\n\n- isp程序默认波特率115200，如果需要更高，这里发送更改命令让isp程序修改串口波特率\n\n- 通过串口发送程序文件到isp程序，isp程序写入到 flash\n\n- 通过串口的 dtr 和 rts 控制芯片的reset和boot引脚来正常启动，不进入 ramrum 模式，而是正常从flash加载程序启动"}, "/news/MaixPy/star_maixpy.html": {"title": "MaixPy 上手指南（避坑）之上手篇", "content": "---\ntitle: MaixPy 上手指南（避坑）之上手篇\nkeywords: MaixPy, K210, Python, MicroPython\ndesc: MaixPy 上手指南（避坑） 之上手篇\ndate: 2022-04-01\ntags: MaixPy, K210\n---\n\n> 作者：Ray（Rui）\n\n拿到热乎的 K210 开发板，如何上手使用。我接触了许许多多的小白开发者后，整理出来的资料和路线，希望可以减少你们遇到的问题，可以更加愉快的使用 K210 进行自己的项目开发。\n\n<!-- more -->\n\n## K210 开发板\n\n市面上有很多中关于 K210 的开发板，但是并不是所有的开发板都是可以使用 MaixPy 进行开发的。毕竟不同厂商使用的摄像头、屏幕、引脚上使用，都是由差异性的。目前支持的使用 MaixPy 开发的板子有 Sipeed 家的 [Maix 系列](/hardware/zh/maix/index.html)。\n\n如果是试用别家的开发板，并不能很好的兼容 MaixPy，存在差异性。\n\n## 开箱\n\n拿到开发板，首先需要根据屏幕和摄像头排线上的丝印提示来安装好，即排线上的数字 “1” 和板子卡座边上引脚丝印 “1” 方位对应接上。上电之后，屏幕上会显示出一个红色的界面这是开发板已经正常启动了。（也可能存在部分丝印印反）\n\n### 首先要安装开发环境：\n\n1. [【安装驱动】](/soft/maixpy/zh/get_started/env_install_driver.html) 根据自己使用的开发进行选择需要按安装驱动\n2. [【更新固件】](/soft/maixpy/zh/get_started/upgrade_maixpy_firmware.html) 确保使用的是最新版本的固件，并学习一下每个固件之间的[差异](/soft/maixpy/zh/get_started/upgrade_maixpy_firmware.html#固件命名说明)\n3. [【安装 MaixPy IDE】](/soft/maixpy/zh/get_started/env_maixpyide.html)\n\n如果安装驱动的时候出现安装失败，或者是安装驱动之后，电脑上没有显示 COM 口的，就需要更新一下系统或者是检查一下自己的系统是不是正版的了。因为有部分的盗版系统安装不上驱动，或者是安装驱动之后并显示。或者通过换 USB 口进行连接，也许就可以检测到开发板\n\n### 运行代码检测摄像头\n\n将开发板接到电脑上，打开 MaixPy IDE，运行打开的例程代码，检查自己的屏幕和摄像头是否正确连接上了。如果运行例程代码之后，并没有图像出现来屏幕和 IDE 上时，可能摄像头接反了。\n\n## 开始学习使用\n\n开始使用 K210 之前，一定要学习 Python，如果你连 Python 都不会的，就不要继续往下走，可以快速的过一遍 [Python](/soft/maixpy3/zh/origin/python.html) 的语法和使用，一定要会 Python !一定要会 Python !一定要会 Python !\n\n现在就当你懂 Python 了，这是就可以开始看 MaixPy 文档中的入门指南，进行对于 MaixPy 的使用和 K210 的基本了解。\n\n【更多功能应用】中将有 MaixPy 更多的使用案例和使用方式，一定要确保自己已经对应入门教程中内容已经了解和掌握了再去看，否则你在学习的时候还是会一脸懵逼，不知所云。\n\n## 获取 AI 模型文件\n\n在【更多功能应用】中是有讲述如何运行神经网络模型，也知道怎么去获取示例中的模型文件，但是少了如何获取机器码这个操作，这里详细的讲述一下何如获取机器码。\n\n1. 将 [key_gen.bin](https://dl.sipeed.com/fileList/MaixHub_Tools/key_gen_v1.2.bin) 这个固件通过 Kflash 烧录到开发板上。烧录这个机器码固件之后，开发板是处于一个不能使用的状态，上电屏幕只会变成一个白屏。\n2. 这时将开发板通过 USB 连接到电脑上，利用[【串口连接】](/soft/maixpy/zh/get_started/env_serial_tools.html)中的方式来连接开发板。注：IDE 中的串口终端和 IDE 的连接方式相对独立的，而且串口不能通过多种方式进行连接\n3. 利用串口软件连接上开发板，这时按下开发板上的 reset 的按键，就会出现一串字符在终端窗口上，这就机器码。如果机器码\n\n> 推荐使用 IDE 中的 串口终端进行查看，这个相对别的软件更加适合 K210\n\n机器码是一机一码的一种加密方式，用于模型文件的加密。如果使用别的机器码去加密或者下载以 smodel 为文件后缀的模型文件，开发板是无法使用该模型文件的。"}}